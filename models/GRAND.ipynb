{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28530,"status":"ok","timestamp":1643228423936,"user":{"displayName":"Hedi Xia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04468829591218008218"},"user_tz":480},"id":"_NuIEnJ4HmWm","outputId":"2fea4712-4112-4c0f-c65e-35cca06d75b9"},"outputs":[],"source":["! pip install ogb \n","! pip install torch_geometric \n","! pip install torch_sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","! pip install xhd_source \n","! pip install torch_scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","! pip install torch_cluster -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","! pip install pykeops\n","! pip install torchsde\n","! pip install torchdiffeq "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2032,"status":"ok","timestamp":1643228425964,"user":{"displayName":"Hedi Xia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04468829591218008218"},"user_tz":480},"id":"Jn4Tw-9GJhME"},"outputs":[],"source":["import argparse\n","import numpy as np\n","import torch\n","from torch_geometric.nn import GCNConv, ChebConv  # noqa\n","import torch.nn.functional as F\n","import time\n","from ogb.nodeproppred import Evaluator\n","import os\n","import random\n","import time\n","import fcntl\n","from xhd_source.notebook_argparse import ArgumentParser\n","\n","import sys\n","sys.path.append('../../')\n","import proxNode.lib.prox.odeprox as odeprox\n","import proxNode.lib.prox.adjoint as adjoint\n","\n","from proxNode.lib.grand.data import get_dataset, set_train_val_test_split\n","from proxNode.lib.grand.GNN import GNN\n","from proxNode.lib.grand.GNN_early import GNNEarly\n","from proxNode.lib.grand.best_params import best_params_dict\n","from proxNode.lib.recorder import Recorder"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":679,"status":"ok","timestamp":1643228426641,"user":{"displayName":"Hedi Xia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04468829591218008218"},"user_tz":480},"id":"uzp9SACTJGcf"},"outputs":[],"source":["\n","\n","def get_optimizer(name, parameters, lr, weight_decay=0):\n","    if name == 'sgd':\n","        return torch.optim.SGD(parameters, lr=lr, weight_decay=weight_decay)\n","    elif name == 'rmsprop':\n","        return torch.optim.RMSprop(parameters, lr=lr, weight_decay=weight_decay)\n","    elif name == 'adagrad':\n","        return torch.optim.Adagrad(parameters, lr=lr, weight_decay=weight_decay)\n","    elif name == 'adam':\n","        return torch.optim.Adam(parameters, lr=lr, weight_decay=weight_decay)\n","    elif name == 'adamax':\n","        return torch.optim.Adamax(parameters, lr=lr, weight_decay=weight_decay)\n","    else:\n","        raise Exception(\"Unsupported optimizer: {}\".format(name))\n","\n","\n","def add_labels(feat, labels, idx, num_classes, device):\n","    onehot = torch.zeros([feat.shape[0], num_classes]).to(device)\n","    if idx.dtype == torch.bool:\n","        idx = torch.where(idx)[0]  # convert mask to linear index\n","    onehot[idx, labels.squeeze()[idx]] = 1\n","\n","    return torch.cat([feat, onehot], dim=-1)\n","\n","\n","def get_label_masks(data, mask_rate=0.5):\n","    \"\"\"\n","    when using labels as features need to split training nodes into training and prediction\n","    \"\"\"\n","\n","    if data.train_mask.dtype == torch.bool:\n","        idx = torch.where(data.train_mask)[0]\n","    else:\n","        idx = data.train_mask\n","    mask = torch.rand(idx.shape) < mask_rate\n","    train_label_idx = idx[mask]\n","    train_pred_idx = idx[~mask]\n","    return train_label_idx, train_pred_idx\n","\n","\n","def train(model, optimizer, data, rec=None):\n","    model.train()\n","    optimizer.zero_grad()\n","    feat = data.x\n","    if model.opt['use_labels']:\n","        train_label_idx, train_pred_idx = get_label_masks(data, model.opt['label_rate'])\n","\n","        feat = add_labels(feat, data.y, train_label_idx, model.num_classes, model.device)\n","    else:\n","        train_pred_idx = data.train_mask\n","\n","    out = model(feat)\n","    if model.opt['dataset'] == 'ogbn-arxiv':\n","        lf = torch.nn.functional.nll_loss\n","        loss = lf(out.log_softmax(dim=-1)[data.train_mask], data.y.squeeze(1)[data.train_mask])\n","    else:\n","        lf = torch.nn.CrossEntropyLoss()\n","        loss = lf(out[data.train_mask], data.y.squeeze()[data.train_mask])\n","    if model.odeblock.nreg > 0:  # add regularisation - slower for small data, but faster and better performance for large data\n","        reg_states = tuple(torch.mean(rs) for rs in model.reg_states)\n","        regularization_coeffs = model.regularization_coeffs\n","\n","        reg_loss = sum(\n","            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n","        )\n","        loss = loss + reg_loss\n","\n","    model.fm.update(model.getNFE())\n","    model.resetNFE()\n","    loss.backward()\n","    optimizer.step()\n","    model.bm.update(model.getNFE())\n","    model.resetNFE()\n","    return loss.item()\n","\n","\n","@torch.no_grad()\n","def test(model, data, opt=None):  # opt required for runtime polymorphism\n","    model.eval()\n","    feat = data.x\n","    # import pdb; pdb.set_trace()\n","    if model.opt['use_labels']:\n","        feat = add_labels(feat, data.y, data.train_mask, model.num_classes, model.device)\n","    logits, accs = model(feat), []\n","    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n","        pred = logits[mask].max(1)[1]\n","        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n","        accs.append(acc)\n","    return accs\n","\n","\n","def print_model_params(model):\n","    print(model)\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            print(name)\n","            print(param.data.shape)\n","\n","\n","@torch.no_grad()\n","def test_OGB(model, data, opt):\n","    if opt['dataset'] == 'ogbn-arxiv':\n","        name = 'ogbn-arxiv'\n","\n","    feat = data.x\n","    if model.opt['use_labels']:\n","        feat = add_labels(feat, data.y, data.train_mask, model.num_classes, model.device)\n","\n","    evaluator = Evaluator(name=name)\n","    model.eval()\n","\n","    out = model(feat).log_softmax(dim=-1)\n","    y_pred = out.argmax(dim=-1, keepdim=True)\n","\n","    train_acc = evaluator.eval({\n","        'y_true': data.y[data.train_mask],\n","        'y_pred': y_pred[data.train_mask],\n","    })['acc']\n","    valid_acc = evaluator.eval({\n","        'y_true': data.y[data.val_mask],\n","        'y_pred': y_pred[data.val_mask],\n","    })['acc']\n","    test_acc = evaluator.eval({\n","        'y_true': data.y[data.test_mask],\n","        'y_pred': y_pred[data.test_mask],\n","    })['acc']\n","\n","    return train_acc, valid_acc, test_acc\n","\n","\n","def main(cmd_opt, rec):\n","    best_opt = best_params_dict[cmd_opt['dataset']]\n","    opt = {**cmd_opt, **best_opt}\n","    opt['block'] = cmd_opt['block']\n","    opt['function'] = cmd_opt['function']\n","    opt['add_source'] = cmd_opt['add_source']\n","    opt['time'] = cmd_opt['time']\n","    opt['adjoint'] = cmd_opt['adjoint']\n","    opt['max_nfe'] = cmd_opt['max_nfe']\n","    opt['tol_scale'] = cmd_opt['tol_scale']\n","    opt['method'] = cmd_opt['adjoint_method']\n","    opt['adjoint_method'] = cmd_opt['adjoint_method']\n","    opt['tol_scale_adjoint'] = cmd_opt['tol_scale_adjoint']\n","    opt['epoch'] = cmd_opt['epoch']\n","    \n","    np.random.seed(opt['seed'])\n","    torch.manual_seed(opt['seed'])\n","    random.seed(opt['seed'])\n","    np.random.RandomState(opt['seed'])\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(opt['seed'])\n","\n","    dataset = get_dataset(opt, '../data', opt['not_lcc'])\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    # import pdb; pdb.set_trace()\n","    model = GNN(opt, dataset, device).to(device) if opt[\"no_early\"] else GNNEarly(opt, dataset, device).to(device)\n","    if opt['prox']:\n","        odeint = cmd_opt['odeprox']\n","        model.odeblock.train_integrator = odeint \n","        model.odeblock.test_integrator = odeint\n","\n","    print('atol: {}, rtol {}'.format(model.odeblock.atol, model.odeblock.rtol))\n","    #print(opt)\n","    if not opt['planetoid_split'] and opt['dataset'] in ['Cora', 'Citeseer', 'Pubmed']:\n","        dataset.data = set_train_val_test_split(opt['seed'], dataset.data,\n","                                                num_development=5000 if opt[\"dataset\"] == \"CoauthorCS\" else 1500,\n","                                                num_per_class=opt['num_train_per_class'])  # Tan TODO\n","    # todo for some reason the submodule parameters inside the attention module don't show up when running on GPU.\n","    data = dataset.data.to(device)\n","    noise = torch.randn_like(data.x) * opt['noise']\n","    if opt['noise_pos'] == 'test':\n","        noise *= (~data.train_mask)[:, None]\n","    data.x += noise\n","    parameters = [p for p in model.parameters() if p.requires_grad]\n","    #print_model_params(model)\n","    optimizer = get_optimizer(opt['optimizer'], parameters, lr=opt['lr'], weight_decay=opt['decay'])\n","    best_time = val_acc = test_acc = train_acc = best_epoch = 0\n","    this_test = test_OGB if opt['dataset'] == 'ogbn-arxiv' else test\n","    for epoch in range(1, opt['epoch']):\n","        start_time = time.time()\n","\n","        tmp_train_acc, tmp_val_acc, tmp_test_acc = this_test(model, data, opt)\n","        loss = train(model, optimizer, data)\n","\n","        if tmp_val_acc > val_acc:\n","            best_epoch = epoch\n","            train_acc = tmp_train_acc\n","            val_acc = tmp_val_acc\n","            test_acc = tmp_test_acc\n","            best_time = opt['time']\n","        if not opt['no_early'] and model.odeblock.test_integrator.solver.best_val > val_acc:\n","            best_epoch = epoch\n","            val_acc = model.odeblock.test_integrator.solver.best_val\n","            test_acc = model.odeblock.test_integrator.solver.best_test\n","            train_acc = model.odeblock.test_integrator.solver.best_train\n","            best_time = model.odeblock.test_integrator.solver.best_time\n","\n","        log = 'Epoch: {:03d}, Runtime {:03f}, Loss {:03f}, forward nfe {:d}, backward nfe {:d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}, Best time: {:.4f}'\n","        fnfe = model.fm.sum\n","        bnfe = model.bm.sum\n","        print(\n","            log.format(epoch, time.time() - start_time, loss, fnfe, bnfe, train_acc, val_acc, test_acc,\n","                       best_time))\n","    rec['fnfe'] = fnfe / opt['epoch']\n","    rec['bnfe'] = bnfe / opt['epoch']\n","    rec['trloss'] = loss \n","    rec['tracc'] = train_acc \n","    rec['vaacc'] = val_acc \n","    rec['tsacc'] = test_acc \n","    return train_acc, val_acc, test_acc\n","\n","\n","class KwargsWrapper():\n","    def __init__(self, func, mode='a', **kwargs):\n","        self.kwargs = kwargs\n","        self.mode = mode\n","        self.func = func\n","\n","    def __call__(self, *args, **kwargs):\n","        if self.mode == 'a':\n","            return self.func(*args, **kwargs, **self.kwargs)\n","        elif self.mode == 'r':\n","            return self.func(*args, **self.kwargs)\n","        else:\n","            raise NotImplemented\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":588},"executionInfo":{"elapsed":34936,"status":"error","timestamp":1643228915255,"user":{"displayName":"Hedi Xia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04468829591218008218"},"user_tz":480},"id":"Jte70OU2J5aQ","outputId":"e727b4e9-e60c-46d5-8982-a24c4296e910"},"outputs":[],"source":["if __name__ == '__main__':\n","    parser = ArgumentParser()\n","    parser.add_argument('--noise', type=float, default=0.0)\n","    parser.add_argument('--noise_pos', type=str, help='all, test')\n","    parser.add_argument('--alltime', action='store_true')\n","    parser.add_argument('--allnumtrain', action='store_true')\n","    \n","    parser.add_argument('--use_cora_defaults', action='store_true',\n","                        help='Whether to run with best params for cora. Overrides the choice of dataset')\n","    # data args\n","    parser.add_argument('--dataset', type=str, default='Cora',\n","                        help='Cora, Citeseer, Pubmed, Computers, Photo, CoauthorCS, ogbn-arxiv')\n","    parser.add_argument('--data_norm', type=str, default='rw',\n","                        help='rw for random walk, gcn for symmetric gcn norm')\n","    parser.add_argument('--self_loop_weight', type=float, default=1.0, help='Weight of self-loops.')\n","    parser.add_argument('--use_labels', dest='use_labels', action='store_true', help='Also diffuse labels')\n","    parser.add_argument('--label_rate', type=float, default=0.5,\n","                        help='% of training labels to use when --use_labels is set.')\n","    parser.add_argument('--planetoid_split', action='store_true',\n","                        help='use planetoid splits for Cora/Citeseer/Pubmed')\n","    # GNN args\n","    parser.add_argument('--hidden_dim', type=int, default=16, help='Hidden dimension.')\n","    parser.add_argument('--fc_out', dest='fc_out', action='store_true',\n","                        help='Add a fully connected layer to the decoder.')\n","    parser.add_argument('--input_dropout', type=float, default=0.5, help='Input dropout rate.')\n","    parser.add_argument('--dropout', type=float, default=0.0, help='Dropout rate.')\n","    parser.add_argument(\"--batch_norm\", dest='batch_norm', action='store_true', help='search over reg params')\n","    parser.add_argument('--optimizer', type=str, default='adam', help='One from sgd, rmsprop, adam, adagrad, adamax.')\n","    parser.add_argument('--lr', type=float, default=0.01, help='Learning rate.')\n","    parser.add_argument('--decay', type=float, default=5e-4, help='Weight decay for optimization')\n","    parser.add_argument('--epoch', type=int, default=100, help='Number of training epochs per iteration.')\n","    parser.add_argument('--alpha', type=float, default=1.0, help='Factor in front matrix A.')\n","    parser.add_argument('--alpha_dim', type=str, default='sc', help='choose either scalar (sc) or vector (vc) alpha')\n","    parser.add_argument('--no_alpha_sigmoid', dest='no_alpha_sigmoid', action='store_true',\n","                        help='apply sigmoid before multiplying by alpha')\n","    parser.add_argument('--beta_dim', type=str, default='sc', help='choose either scalar (sc) or vector (vc) beta')\n","    parser.add_argument('--block', type=str, default='constant', help='constant, mixed, attention, hard_attention, SDE')\n","    parser.add_argument('--function', type=str, default='laplacian', help='laplacian, transformer, dorsey, GAT, SDE')\n","    parser.add_argument('--use_mlp', dest='use_mlp', action='store_true',\n","                        help='Add a fully connected layer to the encoder.')\n","    parser.add_argument('--add_source', dest='add_source', action='store_true',\n","                        help='If try get rid of alpha param and the beta*x0 source term')\n","\n","    # ODE args\n","    parser.add_argument('--time', type=float, default=1.0, help='End time of ODE integrator.')\n","    parser.add_argument('--augment', action='store_true',\n","                        help='double the length of the feature vector by appending zeros to stabilist ODE learning')\n","    parser.add_argument('--method', type=str, default='dopri5',\n","                        help=\"set the numerical solver: dopri5, euler, rk4, midpoint\")\n","    parser.add_argument('--step_size', type=float, default=1,\n","                        help='fixed step size when using fixed step solvers e.g. rk4')\n","    parser.add_argument('--max_iters', type=float, default=100, help='maximum number of integration steps')\n","    parser.add_argument(\"--adjoint_method\", type=str, default=\"adaptive_heun\",\n","                        help=\"set the numerical solver for the backward pass: dopri5, euler, rk4, midpoint\")\n","    parser.add_argument('--adjoint', dest='adjoint', action='store_true',\n","                        help='use the adjoint ODE method to reduce memory footprint')\n","    parser.add_argument('--adjoint_step_size', type=float, default=1,\n","                        help='fixed step size when using fixed step adjoint solvers e.g. rk4')\n","    parser.add_argument('--tol_scale', type=float, default=1., help='multiplier for atol and rtol')\n","    parser.add_argument(\"--tol_scale_adjoint\", type=float, default=1.0,\n","                        help=\"multiplier for adjoint_atol and adjoint_rtol\")\n","    parser.add_argument('--ode_blocks', type=int, default=1, help='number of ode blocks to run')\n","    parser.add_argument(\"--max_nfe\", type=int, default=100000,\n","                        help=\"Maximum number of function evaluations in an epoch. Stiff ODEs will hang if not set.\")\n","    parser.add_argument(\"--no_early\", action=\"store_true\",\n","                        help=\"Whether or not to use early stopping of the ODE integrator when testing.\")\n","    parser.add_argument('--earlystopxT', type=float, default=3, help='multiplier for T used to evaluate best model')\n","    parser.add_argument(\"--max_test_steps\", type=int, default=100,\n","                        help=\"Maximum number steps for the dopri5Early test integrator. \"\n","                             \"used if getting OOM errors at test time\")\n","\n","    # Attention args\n","    parser.add_argument('--leaky_relu_slope', type=float, default=0.2,\n","                        help='slope of the negative part of the leaky relu used in attention')\n","    parser.add_argument('--attention_dropout', type=float, default=0., help='dropout of attention weights')\n","    parser.add_argument('--heads', type=int, default=4, help='number of attention heads')\n","    parser.add_argument('--attention_norm_idx', type=int, default=0, help='0 = normalise rows, 1 = normalise cols')\n","    parser.add_argument('--attention_dim', type=int, default=64,\n","                        help='the size to project x to before calculating att scores')\n","    parser.add_argument('--mix_features', dest='mix_features', action='store_true',\n","                        help='apply a feature transformation xW to the ODE')\n","    parser.add_argument('--reweight_attention', dest='reweight_attention', action='store_true',\n","                        help=\"multiply attention scores by edge weights before softmax\")\n","    # regularisation args\n","    parser.add_argument('--jacobian_norm2', type=float, default=None, help=\"int_t ||df/dx||_F^2\")\n","    parser.add_argument('--total_deriv', type=float, default=None, help=\"int_t ||df/dt||^2\")\n","\n","    parser.add_argument('--kinetic_energy', type=float, default=None, help=\"int_t ||f||_2^2\")\n","    parser.add_argument('--directional_penalty', type=float, default=None, help=\"int_t ||(df/dx)^T f||^2\")\n","\n","    # rewiring args\n","    parser.add_argument(\"--not_lcc\", action=\"store_false\", help=\"don't use the largest connected component\")\n","    parser.add_argument('--rewiring', type=str, default=None, help=\"two_hop, gdc\")\n","    parser.add_argument('--gdc_method', type=str, default='ppr', help=\"ppr, heat, coeff\")\n","    parser.add_argument('--gdc_sparsification', type=str, default='topk', help=\"threshold, topk\")\n","    parser.add_argument('--gdc_k', type=int, default=64, help=\"number of neighbours to sparsify to when using topk\")\n","    parser.add_argument('--gdc_threshold', type=float, default=0.0001,\n","                        help=\"obove this edge weight, keep edges when using threshold\")\n","    parser.add_argument('--gdc_avg_degree', type=int, default=64,\n","                        help=\"if gdc_threshold is not given can be calculated by specifying avg degree\")\n","    parser.add_argument('--ppr_alpha', type=float, default=0.05, help=\"teleport probability\")\n","    parser.add_argument('--heat_time', type=float, default=3., help=\"time to run gdc heat kernal diffusion for\")\n","    parser.add_argument('--att_samp_pct', type=float, default=1,\n","                        help=\"float in [0,1). The percentage of edges to retain based on attention scores\")\n","    parser.add_argument('--use_flux', dest='use_flux', action='store_true',\n","                        help='incorporate the feature grad in attention based edge dropout')\n","    parser.add_argument(\"--exact\", action=\"store_true\",\n","                        help=\"for small datasets can do exact diffusion. If dataset is too big for matrix inversion then you can't\")\n","    parser.add_argument(\"--num_train_per_class\", type=int, default=20)\n","    parser.add_argument('--exp_name', type=str, default='../ray_tune', help=\"where to save results\")\n","    parser.add_argument('--seed', type=int, default=0, help='random seed')\n","    parser.add_argument('--num_runs', type=int, default=1, help='number of runs')\n","    parser.add_argument('--lownumtrain', action='store_true')\n","    parser.add_argument('--x0', action='store_true')\n","\n","    # new args\n","    parser.add_argument('--prox', action='store_true')\n","    \n","    \n","    \n","    cmdstr = '--dataset CoauthorCS --num_train_per_class 20 --time 10 --epoch 50 '\n","    cmdstr += '--block constant --function transformer --no_early '\n","    cmdstr += '--prox'\n","    args = parser.parse_args(cmdstr)\n","\n","    opt = vars(args)\n","    adjoint_methods = ['dopri5', 'adaptive_heun', 'dopri8', 'prox']\n","    opt_methods = ['grad_desc', 'fletch_reeves', 'nesterov', 'nesterov_restart', 'lbfgs', 'bbstep']\n","    prox_methods = ['crank_nicolson', 'euler', 'euler3', 'bdf2', 'bdf3', 'bdf4']\n","\n","    opt['adjoint'] = not opt['prox']\n","    opt['adjoint_method'] = adjoint_methods[2]\n","    opt_method = opt_methods[1]\n","    prox_method = prox_methods[0]\n","\n","\n","    start_time = time.time()\n","    rec = Recorder()\n","    mean_rec = Recorder()\n","    dirname = ''\n","    if opt['alltime']:\n","        time_list = [1.0, 4.0, 16.0, 64.0, 128.0, 256.0]  # [1.0, 2.0, 4.0, 8.0, 16.0, 18.3, 32.0, 64.0, 128.0, 256.0]\n","    else:\n","        time_list = [opt['time']]\n","\n","    if opt['allnumtrain']:\n","        ntpc_list = [20, 10, 5, 2, 1]\n","    elif opt['lownumtrain']:\n","        ntpc_list = [2, 1]\n","    else:\n","        ntpc_list = [opt['num_train_per_class']]\n","    x0 = opt['x0']\n","\n","    if opt['prox']:\n","        tolscales = opt['time'] / np.arange(4, 20)\n","        appname = opt_method + '_' + prox_method\n","    else:\n","        tolscales = 2 ** np.arange(9, 10)\n","        appname = opt['adjoint_method']\n","\n","    for tolscale in tolscales:\n","        opt['tol_scale'] = tolscale\n","        opt['tol_scale_adjoint'] = tolscale\n","        rec['prox_int_step'] = tolscale\n","        if opt['prox']:\n","            odeint = adjoint.odeint_adjoint if opt['adjoint'] else odeprox.odeint\n","            opt['odeprox'] = KwargsWrapper(odeint, mode='r', opt_method=opt_method, prox_method=prox_method, int_step=tolscale, opt_step=0.3, options={'tol':1e-6})\n","        for i in range(opt['num_runs']):\n","            run_start_time = time.time()\n","            for t in time_list:\n","                opt['time'] = t\n","                \n","                print('time {} run {}'.format(t, i))\n","                opt['seed'] = i\n","                np.random.seed(opt['seed'])\n","                torch.manual_seed(opt['seed'])\n","                random.seed(opt['seed'])\n","                np.random.RandomState(opt['seed'])\n","                if torch.cuda.is_available():\n","                    torch.cuda.manual_seed_all(opt['seed'])\n","\n","                train_acc_val, val_acc_val, test_acc_val = main(opt, rec)\n","\n","                t_rep = str(int(t)).zfill(3)\n","                rec[t_rep] = test_acc_val\n","                mean_rec[t_rep] = test_acc_val\n","                \n","            rec['#time_elapsed'] = (time.time() - run_start_time) / 3600.0\n","            rec['#x0'] = int(x0)\n","            rec['#ntpc'] = opt['num_train_per_class']\n","            rec['#numruns'] = opt['num_runs']\n","            rec['#runnum'] = i\n","            rec['log10atol'] = np.log10(tolscale) - 7\n","            rec['log10rtol'] = np.log10(tolscale) - 9\n","            \n","            rec.capture(verbose=True)\n","            rec.writecsv(\n","                os.path.join(dirname, 'GNN_{}_{}.csv'.format(opt['dataset'], appname)))\n","\n","        mean_rec['#x0'] = int(x0)\n","        mean_rec['#ntpc'] = opt['num_train_per_class']\n","        mean_rec['#numruns'] = opt['num_runs']\n","        mean_rec.capture(verbose=True)\n","        mean_rec.writecsv(\n","            os.path.join(dirname, 'mean_GNN_{}_{}.csv'.format(opt['dataset'], appname)))\n","        # del opt['odeprox']\n","    time_elapsed = (time.time() - start_time) / 3600.0\n","    print('time elapsed', time_elapsed, 'hours')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1643228468209,"user":{"displayName":"Hedi Xia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04468829591218008218"},"user_tz":480},"id":"x7UlELnJqmuZ"},"outputs":[],"source":["!nvidia-smi"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of Copy of Copy of GRAND.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
